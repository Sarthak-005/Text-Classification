{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff4824b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tree import *\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import Tree\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import string \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "794a2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(text):\n",
    "    \n",
    "    chunks=[]\n",
    "    chunks=(list(ne_chunk(pos_tag(word_tokenize(text)))))\n",
    "    for i in chunks:\n",
    "        \n",
    "        if type(i)==Tree:\n",
    "            if i.label() == \"GPE\":\n",
    "                j = i.leaves()\n",
    "                if len(j)>1:\n",
    "                    gpe = \"_\".join([term for term,pos in j])\n",
    "                    text = re.sub(rf'{j[1][0]}',gpe,text, flags=re.MULTILINE)           \n",
    "                    text = re.sub(rf'\\b{j[0][0]}\\b',\"\",text, flags=re.MULTILINE) \n",
    "            if i.label()==\"PERSON\":\n",
    "                for term,pog in i.leaves():\n",
    "                    text = re.sub(re.escape(term),\"\",text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def subject(text):\n",
    "    text = re.sub(r\"Re:\",\" \",text,flags=re.MULTILINE)\n",
    "    line = re.findall(r'^Subject.*$',text, re.MULTILINE)\n",
    "    sub = line[0]\n",
    "    #sub = sub[8:]   #Truncate Subject\n",
    "    for i in string.punctuation:   #remove all the non-alphanumeric\n",
    "        sub = sub.replace(i,\" \")\n",
    "    sub = re.sub(r\"Re:\",\" \",sub, re.MULTILINE) #removing Re\n",
    "    sub = sub.lower()  #lower-casing\n",
    "    sub = re.sub(r'Subject',\" \",sub,flags=re.IGNORECASE)  #removing subject\n",
    "    sub=re.sub(r\"\\s+\", \" \", sub)#removing space\n",
    "    sub=re.sub(r\"^\\s+\", \"\", sub)#removing leading space\n",
    "    return sub\n",
    "\n",
    "def email_preprocess(text):\n",
    "    emails = re.findall(r\"@[A-Za-z0-9\\.\\-+_]+\\.[A-Za-z]+\", text)\n",
    "    all_IDs=[]\n",
    "    for email in emails:\n",
    "        ID = email.split('@')[-1].split('.')\n",
    "        if 'com' in ID:\n",
    "            ID.remove('com')\n",
    "        if 'Com' in ID:\n",
    "            ID.remove('Com')\n",
    "        rm = [x for x in ID if len(x)<=2 ]\n",
    "        new_ID = [x for x in ID if x not in rm]\n",
    "        all_IDs.extend(new_ID)\n",
    "    final_IDs = []\n",
    "    [final_IDs.append(x) for x in all_IDs if x not in final_IDs]\n",
    "    #print(final_IDs)  \n",
    "    return final_IDs\n",
    "\n",
    "def Text_preprocessing(file):\n",
    "    f1 = open(file,\"r+\")\n",
    "    txt_class = file.split(\"_\")[0]\n",
    "        \n",
    "    txt = f1.read()\n",
    "        \n",
    "    #processing emails    \n",
    "    email = email_preprocess(txt)\n",
    "    if email is not None:\n",
    "        email = \" \".join(email)\n",
    "        \n",
    "    #remove emails\n",
    "    txt = re.sub(\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\",' ',txt)\n",
    "    \n",
    "    #store subject and remove it from text  \n",
    "    sbj = subject(txt)\n",
    "    txt = re.sub(r'Subject.*$',\" \",txt,flags=re.MULTILINE)\n",
    "    sbj = re.sub('\\W+',' ', sbj)\n",
    "    \n",
    "    \n",
    "    #remove sentences starting with From and Write to\n",
    "    txt = re.sub(r\"Write to:.*$\",\"\",txt, flags=re.MULTILINE)           \n",
    "    txt = re.sub(r\"From:.*$\",\"\",txt, flags=re.MULTILINE)               \n",
    "    txt = re.sub(r\"or:\",\" \",txt,flags=re.MULTILINE)\n",
    "    \n",
    "    #Task-6,7,8\n",
    "    txt = re.sub(r\"<.*>\",\"\",txt, flags=re.MULTILINE) \n",
    "    txt = re.sub(r\"\\(.*\\)\",\"\",txt,flags=re.MULTILINE)\n",
    "    txt = re.sub(r\"\\([^()]*\\)\", \"\", txt) \n",
    "    txt = re.sub(r\"[\\n\\t\\-\\\\\\/]\",\" \",txt, flags=re.MULTILINE)\n",
    "    \n",
    "    #Task-9\n",
    "    txt = re.sub(r'\\w+:\\s?','',txt)\n",
    "    txt = re.sub('\\s+',' ',txt)\n",
    "    #txt = re.sub('^>+','',txt)\n",
    "    txt = re.sub('\\W+',' ', txt )  \n",
    "    \n",
    " \n",
    "    #Task -10\n",
    "    txt = re.sub(r\"won\\'t\", \"will not\", txt)\n",
    "    txt = re.sub(r\"can\\'t\", \"can not\", txt)\n",
    "\n",
    "    # general\n",
    "    txt = re.sub(r\"n\\'t\", \" not\", txt)\n",
    "    txt = re.sub(r\"\\'re\", \" are\", txt)\n",
    "    txt = re.sub(r\"\\'s\", \" is\", txt)\n",
    "    txt = re.sub(r\"\\'d\", \" would\", txt)\n",
    "    txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
    "    txt = re.sub(r\"\\'t\", \" not\", txt)\n",
    "    txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
    "    txt = re.sub(r\"\\'m\", \" am\", txt)\n",
    "    \n",
    "    #Task-11\n",
    "    txt = chunking(txt)\n",
    "                \n",
    "        #removing numbers\n",
    "     ## Delete Number \n",
    "        \n",
    "    txt = re.sub('[0-9\\n]',' ',txt)\n",
    "        \n",
    "    \n",
    "    #14\n",
    "    txt = re.sub(r\"\\b_([a-zA-z]+)_\\b\",r\"\\1\",txt)\n",
    "    \n",
    "    txt = re.sub(r\"\\b_([a-zA-z]+)\\b\",r\"\\1\",txt) #replace_word to word\n",
    "    txt = re.sub(r\"\\b([a-zA-z]+)_\\b\",r\"\\1\",txt)\n",
    "    \n",
    "    #15\n",
    "    txt = re.sub(r\"\\b[a-zA-Z]{1}_([a-zA-Z]+)\",r\"\\1\",txt) #d_berlin to berlin\n",
    "    txt = re.sub(r\"\\b[a-zA-Z]{2}_([a-zA-Z]+)\",r\"\\1\",txt) #mr_cat to cat\n",
    "    \n",
    "    #16\n",
    "    txt = txt.lower()      #lower case\n",
    "    txt = re.sub(r'\\b\\w{1,2}\\b',\" \",txt) #remove words <2\n",
    "    txt = re.sub(r\"\\b\\w{15,}\\b\",\" \",txt) #remove words >15\n",
    "\n",
    "    txt = re.sub(r\"[^a-zA-Z_]\",\" \",txt)  #keep only alphabets and _\n",
    "        \n",
    "    #17                                        \n",
    "    txt = re.sub(r\" {2,}\", \" \", txt, flags=re.MULTILINE) # REMOVE THE EXTRA SPACES\n",
    "    \n",
    "    return email, sbj, txt, txt_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86b8fc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 18828/18828 [23:16<00:00, 13.48it/s]\n"
     ]
    }
   ],
   "source": [
    "files_folder = r\"E:/projectss/CNN_with_textdata/documents/\"\n",
    "os.chdir(files_folder)\n",
    "all_emails = []\n",
    "all_subjects = []\n",
    "all_text = []\n",
    "all_classes = []\n",
    "i=0\n",
    "for file in tqdm(os.listdir()): \n",
    "    \n",
    "    preprocessed_emails,subjects,text,txt_class = (Text_preprocessing(file))\n",
    "    all_emails.append(preprocessed_emails)\n",
    "    all_subjects.append(subjects)\n",
    "    all_text.append(text)\n",
    "    all_classes.append(txt_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cadb7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'class':all_classes,'preprocessed_text':all_text,'preprocessed_subject':all_subjects,'preprocessed_emails':all_emails})\n",
    "#print(df.tail())\n",
    "df.to_csv('E:/projectss/CNN_with_textdata/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd906d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
